{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  In this notebook, we will compare AdaSSP to NoisyGD\n",
    "\n",
    "Recall that we have covered SSP and AdaSSP for linear regression the previous notebook.  In this notebook, we will go over the noisy Gradient Descent algorithm that is way more general, and to evaluate its pros and cons. \n",
    "\n",
    "This is another notebook with end-to-end experiments and actual implementation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use the Noisy Gradient Descent algorithm.\n",
    "\n",
    "Let's say we are minimizing a function $f(\\theta)$.  The gradient descent algorithm iteratively run\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta_t \\nabla f(\\theta_t)\n",
    "$$\n",
    "\n",
    "Noisy gradient dsecent is a differentially private algorithm which updates the parameters by \n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta_t \\big(\\nabla f(\\theta_t) + \\textrm{GS}_t \\cdot \\mathcal{N}(0,\\sigma^2 I) \\big)\n",
    "$$\n",
    "where $\\textrm{GS}_t$ is the global sensitivity of $\\nabla f(\\theta_t)$ as we add/remove individual data points (that contribute to $f$).\n",
    "\n",
    "For linear regression:\n",
    "$$f(\\theta) = \\sum_{i=1}^n(y_i - x_i^T\\theta)^2$$\n",
    "\n",
    "Notice that for NoisyGD, the global sensitivity of $f_t$ depends on $\\theta$ and can be calculated by \n",
    "$$\n",
    "\\textrm{GS}_t  =  \\sup_{x\\in\\mathcal{X},y\\in\\mathcal{Y}} \\|x(y - x^T\\theta_t)\\| =  \\|\\mathcal{X}\\|^2\\|\\theta_t\\| + \\|\\mathcal{X}\\|\\|\\mathcal{Y}\\|.\n",
    "$$\n",
    "\n",
    "## The underlying DP mechanism for running NoisyGD for ```niter``` iterations is simply:  composition of ```niter``` Gaussian mechanism.\n",
    "\n",
    "\n",
    "Recall that the standard workflow of autodp is the following:\n",
    "\n",
    "1. Describe this differentially private mechanism in autodp\n",
    "2. Calibrate the parameter of this DP mechanism to achieve a pre-defined budget.\n",
    "2. Implement the algorithm and compare with the non-private baseline on a real dataset.\n",
    "\n",
    "This is what we are going to do. Before that, we will copy the relevant part of the code on SSP and AdaSSP over to have a baseline of comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following four blocks is taken from the SSP vs AdaSSP notebook (you may skip)\n",
    "\n",
    "Things are covered in these blocks include: Code up SSP / AdaSSP,  Load / preprocess data,  set up experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodp.autodp_core import Mechanism\n",
    "from autodp.mechanism_zoo import ExactGaussianMechanism\n",
    "from autodp.transformer_zoo import ComposeGaussian\n",
    "\n",
    "# SSP and AdaSSP\n",
    "\n",
    "class SSP(Mechanism):\n",
    "    def __init__(self,sigma1,sigma2,name='SSP'):\n",
    "        Mechanism.__init__(self)\n",
    "        self.name = name\n",
    "        self.params={'sigma1':sigma1,'sigma2':sigma2}\n",
    "        gm1 = ExactGaussianMechanism(sigma1,name='Release_XTX')\n",
    "        gm2 = ExactGaussianMechanism(sigma2,name='Release_XTy') \n",
    " \n",
    "        # compose them with the transformation: ComposeGaussian.\n",
    "        compose = ComposeGaussian() \n",
    "        mech = compose([gm1, gm2 ], [1,1])\n",
    "        \n",
    "        # Set all representation to be that of SSP\n",
    "        self.set_all_representation(mech)\n",
    "\n",
    "class AdaSSP(Mechanism):\n",
    "    def __init__(self,sigma1,sigma2,sigma3,name='AdaSSP'):\n",
    "        Mechanism.__init__(self)\n",
    "        self.name = name\n",
    "        self.params={'sigma1':sigma1,'sigma2':sigma2,'sigma3':sigma3}\n",
    "        gm1 = ExactGaussianMechanism(sigma1,name='Release_XTX')\n",
    "        gm2 = ExactGaussianMechanism(sigma2,name='Release_XTy') \n",
    "        gm3 = ExactGaussianMechanism(sigma3,name='Release_lambdamin') \n",
    " \n",
    "        # compose them with the transformation: ComposeGaussian.\n",
    "        compose = ComposeGaussian() \n",
    "        mech = compose([gm1, gm2, gm3 ], [1,1,1])\n",
    "        \n",
    "        # Set all representation to be that of SSP\n",
    "        self.set_all_representation(mech)\n",
    "        \n",
    "from autodp.calibrator_zoo import eps_delta_calibrator\n",
    "\n",
    "calibrate = eps_delta_calibrator()\n",
    "\n",
    "class SSP_scale(SSP):\n",
    "    def __init__(self,scale,name='SSP'):\n",
    "        SSP.__init__(self,sigma1=scale*2.0,sigma2=scale*3.0,name=name)\n",
    "\n",
    "class AdaSSP_scale(AdaSSP):\n",
    "    def __init__(self,scale,name='AdaSSP'):\n",
    "        AdaSSP.__init__(self,sigma1=scale*2.0,sigma2=scale*3.0,sigma3=scale*5.0,name=name)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a regression dataset.\n",
      "Features are:  ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "The label is:  ['MedHouseVal']\n",
      "The non-private baseline has an MSE of  0.6053084771039058\n",
      "The trivial baseline has an MSE of  1.331547558951559\n"
     ]
    }
   ],
   "source": [
    "# Load data, preprocessing to construct valid bounds.\n",
    "# Finding non-private baseline\n",
    "\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "\n",
    "dataset = sklearn.datasets.fetch_california_housing()\n",
    "print('This is a regression dataset.')\n",
    "print('Features are: ', dataset.feature_names)\n",
    "print('The label is: ', dataset.target_names)\n",
    "\n",
    "# Let's extract the relevant information from the sklearn dataset object\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "dim = X.shape[1]\n",
    "n = X.shape[0]\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Rescaling the feature vectors by their natural ranges (independent to the data)\n",
    "X = X @ np.diag(1./np.array([10,50,100,40,40000,1000,50,100]))\n",
    "# This is to ensure that each feature is of the similar scale\n",
    "\n",
    "# the following bounds are chosen independent to the data\n",
    "x_bound = 1\n",
    "y_bound = 5\n",
    "\n",
    "# Preprocess the feature vector such that the norm is fixed at 5\n",
    "X = x_bound*preprocessing.normalize(X, norm='l2')\n",
    "# Second, clip label. We also need the labels to be bounded between [0,y_bound]\n",
    "y = np.clip(y,0,y_bound)\n",
    "\n",
    "# Now let's construct the sufficient statistics and get the non-private baselines.\n",
    "XTX = X.T@X\n",
    "XTy = X.T@y\n",
    "\n",
    "# Non-private linear regression\n",
    "theta_nonprivate = np.linalg.solve(XTX,XTy)\n",
    "\n",
    "# Let's define the performance metric ---  MSE (mean square error) on the training data\n",
    "\n",
    "def MSE(theta):\n",
    "    return np.linalg.norm(y - X@theta)**2 / n\n",
    " \n",
    "MSE_nonprivate = MSE(theta_nonprivate)\n",
    "\n",
    "# Let's also add a trivial baseline for just outputting the mean\n",
    "MSE_trivial = 1/n*np.linalg.norm(y-np.mean(y))**2\n",
    "\n",
    "print('The non-private baseline has an MSE of ', MSE_nonprivate)\n",
    "print('The trivial baseline has an MSE of ', MSE_trivial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual implementation of SSP and AdaSSP\n",
    "\n",
    "def run_SSP(sigma1,sigma2):\n",
    "    # GM1\n",
    "    Z = x_bound**2 * sigma1 * np.random.normal(size=XTX.shape)\n",
    "    Z_analyzegauss = np.triu(Z) + np.triu(Z,k=1).T \n",
    "    hatXTX = XTX +Z_analyzegauss\n",
    "    \n",
    "    # GM2\n",
    "    hatXTy = XTy + x_bound*y_bound * sigma2 * np.random.normal(size=XTy.shape)\n",
    "    \n",
    "    theta_ssp = np.linalg.solve(hatXTX,hatXTy)\n",
    "    return theta_ssp\n",
    "\n",
    "def run_AdaSSP(sigma1,sigma2,sigma3):\n",
    "    \n",
    "    # GM1\n",
    "    Z = x_bound**2 * sigma1 * np.random.normal(size=XTX.shape)\n",
    "    Z_analyzegauss = np.triu(Z) + np.triu(Z,k=1).T \n",
    "    hatXTX = XTX + Z_analyzegauss\n",
    "    \n",
    "    # GM2\n",
    "    hatXTy = XTy + x_bound*y_bound * sigma2 * np.random.normal(size=XTy.shape)\n",
    "    \n",
    "    # GM3\n",
    "    u,s,vT = np.linalg.svd(XTX)\n",
    "    \n",
    "    lambdamin = s[-1] + x_bound**2 * sigma3 * np.random.normal(size=1) \n",
    "    \n",
    "    lambdamin_lowerbound = max(0,lambdamin - x_bound**2 * sigma3*1.96)\n",
    "    lamb = max(0,np.sqrt(dim) * sigma1 * x_bound**2 * 1.96-lambdamin_lowerbound)\n",
    "    \n",
    "    theta_adassp = np.linalg.solve(hatXTX  + lamb*np.eye(dim),hatXTy)\n",
    "    \n",
    "    return theta_adassp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments and plot the results\n",
    "\n",
    "# Run experiments\n",
    "num = 50\n",
    "rep = 20\n",
    "\n",
    "delta_budget = 1e-6\n",
    "eps_budget_list = np.linspace(0.1,2,num)\n",
    "\n",
    "# Place holder to save the result.\n",
    "MSE_list = np.zeros(shape=(num,rep,2))\n",
    "\n",
    "\n",
    "for i,eps in enumerate(eps_budget_list):\n",
    "    \n",
    "    mech_ssp = calibrate(SSP_scale,eps,delta_budget,[0,100])\n",
    "    mech_adassp = calibrate(AdaSSP_scale,eps,delta_budget,[0,100])\n",
    "    \n",
    "    for j in range(rep):\n",
    "        MSE_list[i,j,0] = MSE(run_SSP(mech_ssp.params['sigma1'],mech_ssp.params['sigma2']))\n",
    "        MSE_list[i,j,1] = MSE(run_AdaSSP(mech_adassp.params['sigma1'],\n",
    "                                         mech_adassp.params['sigma2'],\n",
    "                                         mech_adassp.params['sigma3']))\n",
    "\n",
    "avg_MSE_list = np.squeeze(np.mean(MSE_list,axis=1))\n",
    "med_MSE_list = np.squeeze(np.median(MSE_list,axis=1))\n",
    "\n",
    "# Let's also plot the median\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(eps_budget_list,med_MSE_list[:,0],'o-')\n",
    "plt.plot(eps_budget_list,med_MSE_list[:,1],'s-')\n",
    "plt.plot(eps_budget_list,MSE_nonprivate*np.ones(shape=(num,1)),'k--')\n",
    "plt.plot(eps_budget_list,MSE_trivial*np.ones(shape=(num,1)),'r--')\n",
    "plt.ylim([0.6,1.4])\n",
    "\n",
    "plt.legend(['SSP','AdaSSP','Nonprivate','trivial'])\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('median-MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do these methods compare to NoisyGD?\n",
    "\n",
    "We will run NoisyGD online and then plot the accuracy and the incurred privacy losses on the same figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's first implement NoisyGD from scratch and represent it as a ```Mechanism``` in ```autodp```.\n",
    "\n",
    "We will start with the autodp representation of NoisyGD, which is a straightforward composition of Gaussian mechanisms. Then we will implement the algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodp.mechanism_zoo import GaussianMechanism\n",
    "from autodp.transformer_zoo import ComposeGaussian\n",
    "\n",
    "\n",
    "# The autodp Mechanism representation of NoisyGD is the following\n",
    "class NoisyGD_mech(Mechanism):\n",
    "    def __init__(self,sigma,coeff,name='NoisyGD'):\n",
    "        Mechanism.__init__(self)\n",
    "        self.name = name\n",
    "        self.params={'sigma':sigma,'coeff':coeff}\n",
    "        gm = GaussianMechanism(sigma,name='Release_gradient')\n",
    "        # compose them with the transformation: ComposeGaussian.\n",
    "        compose = ComposeGaussian() \n",
    "        mech = compose([gm], [coeff])\n",
    "        \n",
    "        self.set_all_representation(mech)        \n",
    "\n",
    "        \n",
    "# Now let's actually implement the noisy gradient descent algorithm\n",
    "\n",
    "def gradient(theta):\n",
    "    # the gradient is  sum_i x_i (x_i^T \\theta - y)\n",
    "    # It has a global sensitivity of x_bound^2\\|theta\\|_2 + x_bound * y_bound\n",
    "    # in matrix form it is the following\n",
    "    return XTX@theta - XTy\n",
    "\n",
    "def run_NoisyGD_step(theta,sigma, lr):\n",
    "    GS = x_bound**2 * np.linalg.norm(theta) + x_bound * y_bound\n",
    "    return theta - lr * (gradient(theta)  + GS*sigma*np.random.normal(size=theta.shape))\n",
    "\n",
    "# function to run NoisyGD \n",
    "def run_NoisyGD(sigma,lr,niter, log_gap = 10):\n",
    "    theta_GD = np.zeros_like(theta_nonprivate)\n",
    "    MSE_GD = []\n",
    "    eps_GD = []\n",
    "    for i in range(niter):\n",
    "        theta_GD = run_NoisyGD_step(theta_GD,sigma, lr)\n",
    "        if not i%log_gap:    \n",
    "            mech = NoisyGD_mech(sigma,i+1)\n",
    "            eps_GD.append(mech.approxDP(delta))\n",
    "            MSE_GD.append(MSE(theta_GD))\n",
    "    return MSE_GD, eps_GD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How do we choose the hyperparameters for NoisyGD?\n",
    "\n",
    "Our strategy of choosing this hyperparameter is to first set the noise level and the number of iterations. (We can alternatively fix one of these and use autodp's privacy calibrator to determine the other.)  \n",
    "\n",
    "Once we decide on the noise level and the number of iterations, we will choose the learning rate by the optimal theoretical choice.  It requires a data-dependent quantity which we wave our hand  (ideally we can release it also at a small additional cost).\n",
    "\n",
    "We will first compare two regimes:  \n",
    "1. large noise, large number of iterations, small learning rate; \n",
    "2. small noise, small number of iterations, large learning rate.\n",
    "\n",
    "Then we will compare the impact of wiggling the learning rate near the theoretical choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodp.calibrator_zoo import eps_delta_calibrator\n",
    "\n",
    "def find_appropriate_niter(sigma, eps,delta):\n",
    "    # Use autodp calibrator for selecting 'niter'\n",
    "    NoisyGD_fix_sigma = lambda x:  NoisyGD_mech(sigma,x)\n",
    "    calibrate = eps_delta_calibrator()\n",
    "    mech = calibrate(NoisyGD_fix_sigma, eps, delta, [0,100000])\n",
    "    niter = int(np.floor(mech.params['coeff']))\n",
    "    return niter\n",
    "\n",
    "def theoretical_lr_choice(L,dim,sigma,niter):\n",
    "    # L is the gradient lipschitz constant, sigma is the variance of the gradient noise in each coordinate\n",
    "    # niter is the intended number of iterations (the LR is optimized for the point we get when finishing all niter)\n",
    "    return np.minimum(1/L,4/np.sqrt(dim*sigma*lambdamax*niter))\n",
    "\n",
    "\n",
    "# Large noise\n",
    "sigma = 500.0\n",
    "eps = 2.0\n",
    "delta = 1e-6\n",
    "niter = find_appropriate_niter(sigma, eps,delta)\n",
    "\n",
    "print(niter)\n",
    "\n",
    "#niter = 50000 # \n",
    "\n",
    "# find the theoretical learning rate choice by first working out the strong smoothness property\n",
    "u,s,vT = np.linalg.svd(XTX)    \n",
    "lambdamax = s[0]\n",
    "# find an estimated data-independent typical GS (actual GS is chosen on the fly)\n",
    "GS = x_bound**2 * 1 + x_bound*y_bound\n",
    "\n",
    "lr = theoretical_lr_choice(lambdamax,dim,sigma*GS,niter)\n",
    "#lr = np.minimum(1/lambdamax,np.sqrt(2)/np.sqrt(dim*sigma*lambdamax*niter))\n",
    "# Theoretical choice for GD  (those this is giving GD a bit of unfair advantage because lambdamax is data-dependent)\n",
    "# For a more data-independent choice, one can use x_bound**2 * n,   though n is also data-dependent.\n",
    "MSE_GD1, eps_GD1 = run_NoisyGD(sigma,lr,niter,log_gap=100)\n",
    "\n",
    "# Small noise\n",
    "sigma = 50\n",
    "niter = find_appropriate_niter(sigma, eps,delta)\n",
    "print(niter)\n",
    "# niter = 500\n",
    "lr = theoretical_lr_choice(lambdamax,dim,sigma*GS,niter)\n",
    "MSE_GD2, eps_GD2 = run_NoisyGD(sigma,lr,niter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also plot the median\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(eps_budget_list,med_MSE_list[:,0],'o-')\n",
    "plt.plot(eps_budget_list,med_MSE_list[:,1],'s-')\n",
    "plt.plot(eps_GD1, MSE_GD1,'g.-')\n",
    "plt.plot(eps_GD2, MSE_GD2,'c.-')\n",
    "plt.plot(eps_budget_list,MSE_nonprivate*np.ones(shape=(num,1)),'k--')\n",
    "plt.plot(eps_budget_list,MSE_trivial*np.ones(shape=(num,1)),'r--')\n",
    "plt.ylim([0.6,1.4])\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(['SSP','AdaSSP','NoisyGD-large-noise-more-iter','NoisyGD-small-noise-fewer-iter','Nonprivate','trivial'])\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('median-MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we wiggle the learning rate for a bit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 500.0\n",
    "\n",
    "u,s,vT = np.linalg.svd(XTX)    \n",
    "lambdamax = s[0]\n",
    "niter = 50000\n",
    "\n",
    "lr = 10*theoretical_lr_choice(lambdamax,dim,sigma*GS,niter)\n",
    "# Theoretical choice for GD  (those this is giving GD a bit of unfair advantage because lambdamax is data-dependent)\n",
    "\n",
    "MSE_GD3, eps_GD3 = run_NoisyGD(sigma,lr,niter,log_gap=100)\n",
    "\n",
    "lr = 0.1*theoretical_lr_choice(lambdamax,dim,sigma*GS,niter)\n",
    "# Theoretical choice for GD  (those this is giving GD a bit of unfair advantage because lambdamax is data-dependent)\n",
    "\n",
    "MSE_GD4, eps_GD4 = run_NoisyGD(sigma,lr,niter,log_gap=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also plot the median\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "plt.figure(figsize=(8, 5))\n",
    "# plt.plot(eps_budget_list,med_MSE_list[:,0],'o-')\n",
    "# plt.plot(eps_budget_list,med_MSE_list[:,1],'s-')\n",
    "plt.plot(eps_GD1, MSE_GD1,'g.-')\n",
    "plt.plot(eps_GD3, MSE_GD3,'c--')\n",
    "plt.plot(eps_GD4, MSE_GD4,'m:')\n",
    "plt.plot(eps_budget_list,MSE_nonprivate*np.ones(shape=(num,1)),'k--')\n",
    "plt.plot(eps_budget_list,MSE_trivial*np.ones(shape=(num,1)),'r--')\n",
    "plt.ylim([0.6,1.4])\n",
    "\n",
    "plt.legend(['NoisyGD','NoisyGD-lr*10','NoisyGD-lr/10','Nonprivate','trivial'])\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('median-MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Noisy Gradient Descent is a very competitive algorithm especially when we tune its hyperparameter accurately.  Sometimes it requires data-dependent choices, which could be chosen differentially privately at an additional privacy cost.  Specifically, we find that:\n",
    "\n",
    "1. Using larger noise and many iterations help NoisyGD to perform competitively. \n",
    "2. The algorithm is somewhat sensitive to the choice of the learning rate, and it may either converge slowly or diverge if not chosen appropriately. \n",
    "\n",
    "Overall, for linear regression, I find AdaSSP is still the preferred algorithm. However NoisyGD is a more general algorithm that can be applied to even deep learning.  We see that with appropriately chosen hyperparameters, NoisyGD enjoys some strong adaptivity (e.g., no need to have a bound on $\\theta$), also it converges faster when the data are good almost for free.\n",
    "\n",
    "In terms of privacy accounting and privacy calibration.  The use of ```autodp``` makes it very straightforward to get exactly optimal privacy loss or parameters to achieve a certain privacy budgets for these examples, which could fit into most workflows very easily.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
